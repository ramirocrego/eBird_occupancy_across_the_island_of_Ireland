```{r, message=FALSE, warning=FALSE}
# Single species occupancy model

# Load libraries
library(nimble)
library(coda)
library(mcmcOutput)
library(MCMCvis)
library(tidyverse)
library(reshape)
library(bayestestR)

# Clear environment
rm(list = ls())

# Load data
load("./Data/occ_data14sp.RData")
```

# Occupancy buds model 

```{r, message=FALSE, warning=FALSE}
occmod <- nimbleCode({
  
  mu_a0 <- logit(mean_a0)
  mean_a0 ~ dunif(0,1)

  sig_a0 ~ dunif(0,10)       #SD
  tau_a0 <- pow(sig_a0, -2) #Precision
  
  for (t in 1:nyears) {
    alpha0[t] ~ dnorm(mu_a0, tau_a0)
  }
  
  #Beta0
  mu_b0 <- logit(mean_b0)
  mean_b0 ~ dunif(0,1)
  sig_b0 ~ dunif(0,5)      #SD
  tau_b0 <- pow(sig_b0, -2)    #Precision
  
  for (t in 1:nyears) {
    beta0[t] ~ dnorm(mu_b0, tau_b0)
  }
  
  #Alphas
  for(k in 1:4){            
    alpha[k] ~ dnorm(0, 0.01)        
  }
  
  #Betas
  for(k in 1:6){            
    beta[k] ~ dnorm(0, 0.01) 
  }
  
  #Random effect on site
  for(t in 1:nyears){
    sd_lam[t] ~ dunif(0, 2)      
    tau_lam[t] <- pow(sd_lam[t], -2)     
    for(i in 1:nsites){
      eps_lam[i,t] ~ dnorm(0, tau_lam[t]) # Random effect on site with different variance every year
    }
  }
  
  # Likelihood
  for(t in 1:nyears){  
    for (i in 1:nsites) {
      # True state model for the partially observed true state
      z[i,t] ~ dbern(psi[i,t])		# True occupancy z at site i
      logit(psi[i,t]) <- lpsi.lim[i,t]
      lpsi.lim[i,t] <- min(250, max(-250, lpsi[i,t])) # 'Stabilize' logit
      lpsi[i,t] <- alpha0[t] + alpha[1] * Elev[i] + alpha[2] * WVC[i] + alpha[3] * HMI[i] + alpha[4] * Agri[i] + eps_lam[i,t] 
      
      res[i,t]<-psi[i,t]-z[i,t]
      
      for (j in 1:nvisists[i,t]) { 
        # Observation model for the actual observations
        y[i,j,t] ~ dbern(p.eff[i,j,t])	# Detection-nondetection at i, j and t
        p.eff[i,j,t] <- z[i,t] * p[i,j,t]
        logit(p[i,j,t]) <- beta0[t] + beta[1] * WindSpeed[i,j,t] + beta[2] * HoursofDay[i,j,t] + beta[3] * HoursofDay2[i,j,t] + beta[4] * HMI [i] + beta[5] * EffortMin[i,j,t] + beta[6] * OEI[i,j,t]
      }#j reps
    }# i sites
  }#t years
  
  # Estimate PAO per site
  for (t in 1:nyears){
    n.occ[t] <- sum(z[1:nsites,t])/nsites
  }
  
})


## Create constants, data, and initial values to pass to the model builder
Consts <- list(nsites = nsites, nyears=nyears, nvisists = nvisists)

Data <- list(y = Y[,,,14], # Turdus merula
             EffortMin = EffortMinSc,
             HoursofDay = HoursofDaySc,
             HoursofDay2 = HoursofDaySc*HoursofDaySc,
             WindSpeed = WindSpeedSc,
             OEI = OEISc,
             Elev = EleSc,
             HMI = HMISc,
             WVC = WoodVegCovSc,
             Agri = AgrSc
)

zst <- matrix(1,nrow = nsites, ncol = nyears)
Inits <- list(
  z = zst, 
  mean_a0 = runif(1, 0, 1), 
  mean_b0 = runif(1, 0, 1),
  sig_a0 = runif(1, 0, 1),
  sig_b0 = runif(1, 0, 1),
  alpha = rnorm(4, 0, 1),
  beta = rnorm(6, 0, 1)
)

## Parameters to estimate
params <- c("mu_a0", "sig_a0", "alpha0", "alpha", "mu_b0", "sig_b0", "beta0", "beta", "n.occ", "mean_a0", "mean_b0", "z", "p", "res") 


## Paralell running (for laptop) ----

library(parallel)

this_cluster <- makeCluster(3)

# Create a function with all the needed code

run_MCMC_allcode <- function(Seed, Data, Code, Const, Inits, Params) {
  library(nimble)
  library(coda)
  myModel <- nimbleModel(code = Code,
                         constants = Const, data = Data, inits = Inits,
                         calculate = FALSE)
  
  CmyModel <- compileNimble(myModel)
  Modolconf <- configureMCMC(myModel, monitors = Params)
  myMCMC <- buildMCMC(Modolconf)
  CmyMCMC <- compileNimble(myMCMC)
  
  results <- runMCMC(CmyMCMC,
                     nburnin = 50000,
                     niter = 450000,
                     thin = 400,
                     samples = TRUE,
                     samplesAsCodaMCMC = T,
                     summary = F,
                     WAIC = FALSE,
                     perChainWAIC = FALSE,
                     setSeed = Seed)
  results <- as.mcmc(results)
  return(results)
}

chain_output <- parLapply(cl = this_cluster, X = 1:3,
                          fun = run_MCMC_allcode,
                          Data = Data, Code = occmod,
                          Const = Consts, Inits = Inits, Params = params)

# It's good practice to close the cluster when you're done with it.
stopCluster(this_cluster)

mc <- mcmcOutput(mcmc.list(chain_output))
```

# Model diagnostics

```{r, message=FALSE, warning=FALSE}
diagPlot(mc[c(1:22,8167:8168)])

# Model summary ----
Summary <- summary(mc[c(1:26,8167:8168)],n.eff=TRUE, MCEpc = T) 
parnames <- rownames(Summary)
table1 <- data.frame()
for (i in c(1:26,8167:8168)){ 
  t <- data.frame(Mean = mean(mc[,i]), Median = median(mc[,i]), bayestestR::ci(mc[,i], 0.89), SD= sd(mc[,i]))
  table1 <- rbind(table1, t)
}
table1$Gelman <- Summary$Rhat
table1$MCEpc <- Summary$MCEpc
table1$n.eff <- Summary$n.eff
table1$Param <- parnames
View(table1)

# Check for prior influence on posterior estimates and ensure posterior distribution are not equal to the priors one

niter <- nrow(mc)
PR1 <- runif(niter, 0, 1)
PR2 <- rnorm(niter, 0, 10) 

MCMCtrace(mc, params = 'mean_a0', priors = PR1, pdf = FALSE)
MCMCtrace(mc, params = 'mean_b0', priors = PR1, pdf = FALSE)
MCMCtrace(mc, params = 'alpha', priors = PR2, pdf = FALSE)
MCMCtrace(mc, params = 'beta', priors = PR2, pdf = FALSE)

PR3 <- runif(niter, 0, 10)
MCMCtrace(mc, params = 'sig_a0', priors = PR3, pdf = FALSE)
PR4 <- runif(niter, 0, 5)
MCMCtrace(mc, params = 'sig_b0', priors = PR4, pdf = FALSE)
```

# Goodness-of-fit

```{r, message=FALSE, warning=FALSE}
# Get sites sampled per season
repsampled<-list()
for (y in 1:nyears){
  xA <- Y[,,y,14]
  xB <- which(apply(xA, 1, function(y) !all(is.na(y))), arr.ind = T) # Extract row name that was surveyed each saason
  repsampled[[y]] <- xB
}

#Estimate number of replicates per site per season
n<-list()
for (y in 1:nyears){
  xA <- Y[,,y,14]
  xB <- rowSums(!is.na(xA))
  n[[y]] <- xB
}

# Calculate number of observations per site per season
obs <- array(NA, c(nsites, nyears))
for (y in 1:nyears){
  xA <- Y[,,y,14]
  xB <- rowSums(xA, na.rm=TRUE)
  obs[,y] <- xB 
}

#Estimate number of replicates per site per year
nvisists <- matrix(NA,nsites,nyears)
for(y in 1:nyears){
  temp <- Y[,,y,14]
  nvisists[,y] <- rowSums(!is.na(temp))
} #y

## p sum per site
mp <- mc$p
mz <- mc$z

psSum <- array(NA, dim = c(nrow(mc), nsites, nyears))
for(y in 1:nyears){
  for(i in repsampled[[y]]) {
    for(j in n[[y]][i]){
      psSum[,i,y] <- ifelse(j==1, mp[,i,1:j,y], rowSums(mc$p[,i,1:j,y]))
    }}}

## Calculate Freeman-Tukey Bayesian p-value 
Tobs2 <- Tsim2 <- array(NA, dim = c(nrow(mc), nyears))
for(y in 1:nyears){
  for(iter in 1:nrow(mc)) {
    yTemp <- array(NA, dim = c(nsites, nreps)) # Create empty array to fill with a simulated dataset
    for(i in repsampled[[y]]){
      for(j in 1:n[[y]][i]){
        yTemp[i,j] <- rbinom(1,1,(mp[iter,i,j,y]*mz[iter,i,y])) # Simulate dataset
      }}
    Tobs2[iter,y] <- sum((sqrt(obs[repsampled[[y]],y]) - sqrt(psSum[iter,repsampled[[y]],y] * mz[iter,repsampled[[y]],y]))^2) # Freeman-Tukey statistic for observed data
    ySim <- rowSums(yTemp[repsampled[[y]],], na.rm=TRUE) # Calculate total number of observations from simulated data
    Tsim2[iter,y] <- sum((sqrt(ySim) - sqrt(psSum[iter,repsampled[[y]],y] * mz[iter,repsampled[[y]],y]))^2) # Freeman-Tukey statistic for simulated data
  }
}

Tobs22 <- apply(Tobs2, 1, sum)
Tsim22 <- apply(Tsim2, 1, sum)

MASS::eqscplot(Tobs22, Tsim22, xlim=range(Tobs22, Tsim22), ylim=range(Tobs22, Tsim22), xlab="Observed data", ylab="Simulated data")
abline(0, 1, lwd=2, col='red')
mean(Tsim22 > Tobs22) # the P value
```

# Compare simulated data with naive occupancy

```{r, message=FALSE, warning=FALSE}
SimNaivOcc <- data.frame()
for(iter in 1:nrow(mc)){
  for(y in 1:nyears){
    yTemp <- array(NA, dim = c(nsites, nreps))
    for(i in repsampled[[y]]){
      for(j in 1:n[[y]][i]){
        yTemp[i,j] <- rbinom(1,1,(mp[iter,i,j,y]*mz[iter,i,y]))
      }}
    Sum1<-apply(yTemp, 1, sum,na.rm=TRUE)
    sum2<-replace(Sum1,which(Sum1>0),1)
    sum3<-sum(sum2[repsampled[[y]]])
    temp<- data.frame(y = sum3/length(repsampled[[y]]), Year = y, Sim = iter)
    SimNaivOcc <- rbind(SimNaivOcc, temp)
  }
}

resultssim <- SimNaivOcc %>% group_by(Year, Sim) %>% summarise(mean(y))

finalsim <- resultssim %>% group_by(Year) %>% summarise(mean(`mean(y)`), sd(`mean(y)`)) %>% as.data.frame()
colnames(finalsim) <- c("Year","Mean","SD")

# Naive occupancy
Naiveocc <- data.frame()
x <- Y[,,,14]
for(t in 1:nyears){
  x1 <- data.frame(x[,,t])
  x2 <- x1 |> drop_na(X1)
  sum1<-apply(x2, 1, sum,na.rm=TRUE)
  sum2<-ifelse(sum1 > 0, 1, 0)
  sum3<-sum(sum2)
  temp2<- data.frame(y = sum3/length(sum2), Year = years[t])
  Naiveocc <- rbind(Naiveocc, temp2)
}

table <- cbind(Naiveocc,finalsim)
table$dif <- table$y - table$Mean
table$percDiff <- table$dif * 100/table$y
table
mean(table$dif)
mean(table$percDiff); sd(table$percDiff)
```

```{r, message=FALSE, warning=FALSE}
# Spatial autocorrelation
library(pgirmess)
library(sf)

### Load centroids
Grid <- st_read("./Data/Centroids.shp")
Grid$CellID <- sprintf("%05d", as.numeric(Grid$CellID))
centroid <- left_join(data.frame(CellID = sites), Grid) 
centroid <- st_sf(centroid, geometry = centroid$geometry)
coords <- st_coordinates(centroid)

# Arrange res
resP <- grepl(colnames(mc), pattern = "res")
resP <- mc[,resP]
resp <- array(NA, dim = c(nrow(mc), nsites, nyears))
num <- seq(0,(nsites*nyears), nsites)

for(y in 1:nyears){
  resp[,,y] <- resP[,(1+num[y]):(num[y+1])]
}

output <- NULL
for(y in 1:nyears){
  temp1 <- apply(resp[,,y], 2, mean)
  pgi.cor <- correlog(coords=coords, z=temp1, method="Moran", nbclass = NULL) %>% as.data.frame() %>% mutate(Year = years[y]) %>% filter(dist.class < 200000)
  output <- rbind(output, pgi.cor)
}

mean(output$coef)
sd(output$coef)
range(output$coef)

(MoranIplot <- ggplot(output, aes(x=dist.class/1000, y=coef, colour = as.factor(Year), group = as.factor(Year))) + scale_y_continuous(limits = c(-1,1)) + geom_line() + 
    theme_minimal() + labs(title="Moran's I correlogram", x="Distance (km)", y="Moran's I") + theme(plot.title = element_text(hjust = 0.5), strip.text = element_text(face = "italic")) + guides(colour = guide_legend(title = "Year")) )
```


# Figures

```{r, message=FALSE, warning=FALSE}
### Intercept ----
mubeta0_rowsP <- grepl(colnames(mc), pattern = "^mu_b0")
beta0_rowsP <- grepl(colnames(mc), pattern = "^beta0\\[")
beta1_rowsP <- grepl(colnames(mc), pattern = "^beta\\[")
beta1P <- cbind(mc[,beta1_rowsP])

### Parameter value results ----

# Wind speed
median(beta1P[,1])
ci(beta1P[,1], 0.89)
mean(beta1P[,1] < 0)

# HoursofDay
median(beta1P[,2])
ci(beta1P[,2], 0.89)
mean(beta1P[,2] > 0)

median(beta1P[,3])
ci(beta1P[,3], 0.89)
mean(beta1P[,3] > 0)

# HMI
median(beta1P[,4])
ci(beta1P[,4], 0.89)
mean(beta1P[,4] > 0)

# Effort min
median(beta1P[,5])
ci(beta1P[,5], 0.89)
mean(beta1P[,5] > 0)

# Observer expertice index
median(beta1P[,6])
ci(beta1P[,6], 0.89)
mean(beta1P[,6] > 0)


### Wind ----
range(WindSpeedSc)
windsc <- seq(-1.92,6.2,by=0.1)
wind <- windsc * sdWS + meanWS

PostWind <- array(NA, dim = c(nrow(mc), length(windsc)))
for (i in 1:nrow(mc)){
  PostWind[i,] <- plogis(mc[i,mubeta0_rowsP] + beta1P[i,1] * windsc)
}
plot1 <- data.frame()
for(i in 1:ncol(PostWind)){
  temp <- data.frame(x = wind[i], y = median(PostWind[,i]), yUpper = ci(PostWind[,i], 0.89)$CI_high, yLower = ci(PostWind[,i], 0.89)$CI_low, Covariable = "Wind speed")
  plot1 <- rbind(plot1, temp)
}

### Hour of day ----
range(HoursofDaySc)
hofdsc <- seq(-2,2,by=0.1)
hofd <- hofdsc * sdHD + meanHD

PostHofD <- array(NA, dim = c(nrow(mc), length(hofdsc)))
for (i in 1:nrow(mc)){
  PostHofD [i,] <- plogis(mc[i,mubeta0_rowsP] + beta1P[i,2] * hofdsc + beta1P[i,3] * hofdsc * hofdsc)
}
plot2 <- data.frame()
for(i in 1:ncol(PostHofD)){
  temp <- data.frame(x = hofd[i], y = median(PostHofD [,i]), yUpper = ci(PostHofD [,i], 0.89)$CI_high, yLower = ci(PostHofD [,i], 0.89)$CI_low, Covariable = "Hour of day")
  plot2 <- rbind(plot2, temp)
}

### Human Index ----
range(HMISc)
hmisc <- seq(-2.38,1.9,by=0.1)
hmi <- hmisc * sdHMI + meanHMI

PostHMI <- array(NA, dim = c(nrow(mc), length(hmisc)))
for (i in 1:nrow(mc)){
  PostHMI [i,] <- plogis(mc[i,mubeta0_rowsP] + beta1P[i,4] * hmisc)
}
plot3 <- data.frame()
for(i in 1:ncol(PostHMI)){
  temp <- data.frame(x = hmi[i], y = median(PostHMI [,i]), yUpper = ci(PostHMI [,i], 0.89)$CI_high, yLower = ci(PostHMI [,i], 0.89)$CI_low, Covariable = "HMI")
  plot3 <- rbind(plot3, temp)
}

### Effort Min ----
range(EffortMinSc)
effsc <- seq(-1.09,2.39,by=0.1)
effort<- effsc * sdEM + meanEM

PostEff <- array(NA, dim = c(nrow(mc), length(effsc)))
for (i in 1:nrow(mc)){
  PostEff [i,] <- plogis(mc[i,mubeta0_rowsP] + beta1P[i,5] * effsc)
}
plot4 <- data.frame()
for(i in 1:ncol(PostEff)){
  temp <- data.frame(x = effort[i], y = median(PostEff [,i]), yUpper = ci(PostEff [,i], 0.89)$CI_high, yLower = ci(PostEff [,i], 0.89)$CI_low, Covariable = "Effort min.")
  plot4 <- rbind(plot4, temp)
}

### Observer Expertice Index ----
range(OEISc)
oeisc <- seq(-1.6,7.4, by=0.1)
oei<- oeisc * sdOEI + meanOEI

PostOEI <- array(NA, dim = c(nrow(mc), length(oeisc)))
for (i in 1:nrow(mc)){
  PostOEI [i,] <- plogis(mc[i,mubeta0_rowsP] + beta1P[i,6] * oeisc)
}
plot5 <- data.frame()
for(i in 1:ncol(PostOEI)){
  temp <- data.frame(x = oei[i], y = median(PostOEI [,i]), yUpper = ci(PostOEI [,i], 0.89)$CI_high, yLower = ci(PostOEI [,i], 0.89)$CI_low, Covariable = "OEI")
  plot5 <- rbind(plot5, temp)
}

### Det prob plot ----  

detplot <- rbind(plot1, plot2, plot3, plot4, plot5)

(plotDetProb <- ggplot(detplot, aes(x = x, y = y)) + facet_wrap(.~Covariable, ncol = 5, scales= "free", strip.position = "bottom") + geom_ribbon(aes(ymin = yLower, ymax = yUpper), fill = "grey90") + geom_line(size=1, colour = 'black') +
    theme_minimal() +
    labs(y="Detection probability\n(89% CI)", x = "") +
    theme(strip.placement = "outside",
          axis.text=element_text(size=12), 
          axis.title=element_text(size=16), 
          axis.line=element_line(linewidth=0.5),
          axis.ticks=element_line(linewidth=0.5), 
          strip.text=element_text(size=14), 
          strip.background=element_blank(), 
          panel.border=element_blank(), 
          panel.grid=element_blank(), legend.position = 'none') + scale_color_manual(values = rep('darkgray', 34)))

## Occupancy probability ----

### Intercept ----
mualpha0_rowsP <- grepl(colnames(mc), pattern = "^mu_a0")
alpha0_rowsP <- grepl(colnames(mc), pattern = "^alpha0\\[")
alpha0P <- cbind(mc[,alpha0_rowsP])
alpha1_rowsP <- grepl(colnames(mc), pattern = "^alpha\\[")
alpha1P <- cbind(mc[,alpha1_rowsP])

### Parameter value results ----

# Elevation
median(alpha1P[,1])
ci(alpha1P[,1], 0.89)
mean(alpha1P[,1] < 0)

# WVI
median(alpha1P[,2])
ci(alpha1P[,2], 0.89)
mean(alpha1P[,2] > 0)

# HMI
median(alpha1P[,3])
ci(alpha1P[,3], 0.89)
mean(alpha1P[,3] < 0)

# Agr
median(alpha1P[,4])
ci(alpha1P[,4], 0.89)
mean(alpha1P[,4] > 0)

###  Elevation ----
range(EleSc)
Elesc <- seq(-0.85,6.72,by=0.1)
elev <- Elesc * sdEle + meanEle

PostEle <- array(NA, dim = c(nrow(mc), length(Elesc)))
for (i in 1:nrow(mc)){
  PostEle[i,] <- plogis(mc[i,mualpha0_rowsP] + alpha1P[i,1] * Elesc )
}
plot6 <- data.frame()
for (i in 1:ncol(PostEle)){
  temp <- data.frame(x = elev[i], y = median(PostEle[,i]), yUpper = ci(PostEle[,i], 0.89)$CI_high, yLower = ci(PostEle[,i], 0.89)$CI_low, Covariable = "Elevation (m)")
  plot6 <- rbind(plot6, temp)
}

###  WVC ----
range(WoodVegCovSc)
wvisc <- seq(-1.17,3,by=0.1)
wvi <- wvisc * sdWoodVegCov + meanWoodVegCov

PostWVC <- array(NA, dim = c(nrow(mc), length(wvisc)))
for (i in 1:nrow(mc)){
  PostWVC[i,] <- plogis(mc[i,mualpha0_rowsP] + alpha1P[i,2] * wvisc)
}
plot7 <- data.frame()
for(i in 1:ncol(PostWVC)){
  temp <- data.frame(x = wvi[i], y = median(PostWVC[,i]), yUpper = ci(PostWVC[,i], 0.89)$CI_high, yLower = ci(PostWVC[,i], 0.89)$CI_low, Covariable = "Woody veg. cov.")
  plot7 <- rbind(plot7, temp)
}

###  HMI ----
# range(ArtSc)
# artsc <- seq(-0.8,1.7,by=0.1)
# art <- artsc * sdArtificial + meanArtificial

PostArtOcc <- array(NA, dim = c(nrow(mc), length(hmisc)))
for (i in 1:nrow(mc)){
  PostArtOcc[i,] <- plogis(mc[i,mualpha0_rowsP] + alpha1P[i,3] * hmisc)
}
plot8 <- data.frame()
for (i in 1:ncol(PostArtOcc)){
  temp <- data.frame(x = hmi[i], y = median(PostArtOcc[,i]), yUpper = ci(PostArtOcc[,i], 0.89)$CI_high, yLower = ci(PostArtOcc[,i], 0.89)$CI_low, Covariable = "HMI")
  plot8 <- rbind(plot8, temp)
}

###  Agricultural cover ----
range(AgrSc)
agrsc <- seq(-1.1,1.6,by=0.1)
agr <- agrsc * sdAgriculture + meanAgriculture

PostAgrOcc <- array(NA, dim = c(nrow(mc), length(agrsc)))
for (i in 1:nrow(mc)){
  PostAgrOcc[i,] <- plogis(mc[i,mualpha0_rowsP] + alpha1P[i,4] * agrsc)
}
plot9 <- data.frame()
for (i in 1:ncol(PostAgrOcc)){
  temp <- data.frame(x = agr[i], y = median(PostAgrOcc[,i]), yUpper = ci(PostAgrOcc[,i], 0.89)$CI_high, yLower = ci(PostAgrOcc[,i], 0.89)$CI_low, Covariable = "Agr. land cover")
  plot9 <- rbind(plot9, temp)
}

### Plot occ prob ----

occplot <- rbind(plot6,plot7,plot8, plot9)

(plotOccProb <- ggplot(occplot, aes(x = x, y = y)) + facet_wrap(.~Covariable,ncol = 5, scales= "free", strip.position = "bottom") + geom_ribbon(aes(ymin = yLower, ymax = yUpper), fill = "grey90") + geom_line(size=1, colour = 'black') +
    theme_minimal() +
    labs(y="Occupancy probability\n(89% CI)", x = "") +
    theme(strip.placement = "outside",
          axis.text=element_text(size=12), 
          axis.title=element_text(size=16), 
          axis.line=element_line(linewidth=0.5),
          axis.ticks=element_line(linewidth=0.5), 
          strip.text=element_text(size=14), 
          strip.background=element_blank(), 
          panel.border=element_blank(), 
          panel.grid=element_blank(), legend.position = 'none') + scale_color_manual(values = rep('darkgray', 34)))

## Map prediction ----
library(terra)
library(tmap)
library(pals) # color palette
load("./Data/enviro_variables.RData")
Centroids <- st_read("./Data/Centroids.shp") 

means <- c(meanEle,meanWoodVegCov, meanHMI, meanAgriculture)
sds <- c(sdEle, sdWoodVegCov, sdHMI, sdAgriculture)

env_variables <- select(env_variables, c(ID, Elev, WoodVegCov, HMI, Agriculture))

env_varst <- matrix(NA, nrow = nrow(env_variables), ncol = 4)
for(i in 1:(ncol(env_variables)-1)){
  env_varst[,i] <- (env_variables[,i+1] - means[i]) / sds[i]
}
env_varst <- data.frame(env_varst) 
names(env_varst) <- names(env_variables[,2:ncol(env_variables)])
env_varst$CellID <- env_variables$ID 

#Prediction
PostPred <- plogis(median(mc[i,mualpha0_rowsP]) + median(alpha1P[i,1]) * env_varst$Elev + median(alpha1P[i,2]) * env_varst$WoodVegCov + median(alpha1P[i,3]) * env_varst$HMI +  median(alpha1P[i,4]) * env_varst$Agriculture)

#Map
PostPred<-data.frame(CellID = env_varst$CellID, Occ = PostPred) 
OccPred <- left_join(Centroids, PostPred, by = "CellID") |> drop_na()
canvas <- rast(xmin=374515.5, xmax=723166, 
               ymin=5682953, ymax=6166209, resolution = 1000, crs = "+init=epsg:32629")
STocc <- rasterize(OccPred, canvas, field = "Occ", fun = "mean") 

(OccMap <- tm_shape(STocc) + tm_raster(col.legend = tm_legend("Occupancy prob."), col.scale = tm_scale_continuous(values = viridis(10))) + tm_layout(frame = FALSE, legend.outside = F, legend.position = c("right","bottom"), legend.frame = F, legend.title.size = 1.4, legend.text.size = 1, legend.width = 10, legend.orientation = "landscape") 
)
```

# Model validation

```{r, message=FALSE, warning=FALSE}
## Load CBS observation data ----
CBS <- read.csv("./Data/counts.csv")
visits<- read.csv("./Data/visits.csv")

# Join datasets
fullCBSdata <- left_join(CBS,visits)

# Filter years
SquaresTemp <- visits %>% filter(Season %in% c(2018,2019,2022,2023))
YearsCBS <- unique(SquaresTemp$Season)

## Nr of sites
Squares <- as.factor(sort(unique(SquaresTemp$Square)))
nSquares <- length(Squares)

## Nr of reps
Visits <- c("E","L")

# Arrange data
CBS_BB <- fullCBSdata %>% filter(Season %in% YearsCBS, SpeciesCode == "B.", CQID == 1)

#Sort by year
CBS_BB <- arrange(CBS_BB, Season, Square)

#Generate observation array
yCBS <- array(NA, dim = c(nSquares,2,length(YearsCBS)))
for(t in 1:length(YearsCBS)){
  J <- filter(CBS_BB, Season == YearsCBS[t])
  J <- group_by(J,  Square, Visit) %>% dplyr::summarize(n = n())
  W <- data.frame(rep(Squares, rep(2, nSquares)), rep(Visits, nSquares))
  colnames(W) <- c("Square", "Visit")
  Y <- full_join(W, J, by = c("Square", "Visit"))
  Y$n[is.na(Y$n)] = 0
  X <- split(Y$n, f = Y$Square)
  X <- do.call(rbind, X)
  yCBS[,,t] <- X
}

# Combine to presence absence assuming perfect detection
CBS_BB <- matrix(NA, nrow= nSquares, ncol = length(YearsCBS))
for(t in 1:length(YearsCBS)){
  sum1<-apply(yCBS[,,t], 1, sum, na.rm=T)
  sum2<-ifelse(sum1 > 0, 1, 0)
  CBS_BB[,t] <- sum2
}

# Fill with NA in squares where sampling did not happen
for(t in 1:length(YearsCBS)){
  J1 <- filter(visits, Season == YearsCBS[t] & Visit == Visits[1])
  Sq1 <- sort(unique(J1$Square))
  SQNA1 <- Squares[!Squares %in% Sq1]
  SQNA1 <- as.integer(SQNA1)
  
  J2 <- filter(visits, Season == YearsCBS[t] & Visit == Visits[2])
  Sq2 <- sort(unique(J2$Square))
  SQNA2 <- Squares[!Squares %in% Sq2]
  SQNA2 <- as.integer(SQNA2)
  SQNA <- sort(unique(c(SQNA1,SQNA2)))
  CBS_BB[SQNA,t] <- NA
}

CBS_BB <- data.frame(Squares, CBS_BB)
names(CBS_BB) <- c("Square", YearsCBS)
head(CBS_BB)

# Load squares and covariates raster
Env_raster <- rast("./Data/Env_raster.tif")
plot(Env_raster)
spdf <- st_read("./Data/Squares.shp")
spdf <- st_transform(spdf, crs = 32629)

env_rasterstd <- c((Env_raster$Elev - meanEle)/sdEle,
                   (Env_raster$WoodVegCov - meanWoodVegCov)/sdWoodVegCov,
                   (Env_raster$HMI - meanHMI)/sdHMI,
                   (Env_raster$Agriculture- meanAgriculture))
plot(env_rasterstd)
SquaresED <- extract(env_rasterstd, spdf, fun = mean)
SquaresED$Square <- spdf$Square

# Join datasets
CBS_BB <- left_join(CBS_BB,SquaresED, by = "Square") 

## Predictions ----
PostPred <- matrix(NA, nrow = nrow(CBS_BB), ncol = 4)

for(y in 1:4){
  PostPred[,y] <- plogis(median(alpha0P[,y]) + median(alpha1P[,1]) * CBS_BB$Elev + median(alpha1P[,2]) * CBS_BB$WoodVegCov + median(alpha1P[,3]) * CBS_BB$HMI +  median(alpha1P[,4]) * CBS_BB$Agriculture)
}

## Calculate area under curve ----
library(pROC)
data4roc <- as.data.frame(cbind(c(CBS_BB$`2018`,CBS_BB$`2019`,CBS_BB$`2022`, CBS_BB$`2023`), c(PostPred[,1],PostPred[,2],PostPred[,3],PostPred[,4]))) |> drop_na()

# Overall
roc(data4roc$V1,data4roc$V2, percent=TRUE, plot=TRUE, ci=TRUE)

# Per year
roc(CBS_BB$`2018`,PostPred[,1], percent=TRUE, plot=TRUE, ci=TRUE)
roc(CBS_BB$`2019`,PostPred[,2], percent=TRUE, plot=TRUE, ci=TRUE)
roc(CBS_BB$`2022`,PostPred[,3], percent=TRUE, plot=TRUE, ci=TRUE)
roc(CBS_BB$`2023`,PostPred[,4], percent=TRUE, plot=TRUE, ci=TRUE)
```


